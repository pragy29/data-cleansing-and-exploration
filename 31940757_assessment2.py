# -*- coding: utf-8 -*-
"""31940757_Assessment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ttPRDoGIT4BHCwDz5PxlBYT89ejv94KR

**Importing Libraries**

---
"""

# connect to google drive
from google.colab import drive
drive.mount('/content/drive')

pip install geopandas

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import os
import math as mat
import re
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
import numpy as np
import nltk
from sklearn.linear_model import LogisticRegression
# install geopandas library
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.metrics import mean_squared_error
# Configure visualisations
# %matplotlib inline
mpl.style.use( 'ggplot' )
#Notebook displace setting
from IPython.core.display import HTML
# css = open('style/style-table.css').read() + open('style/style-notebook.css').read()
css = open('/content/drive/Shareddrives/FIT5196_S1_2023/week6/style/style-table.css').read() + open('/content/drive/Shareddrives/FIT5196_S1_2023/week6/style/style-notebook.css').read()

HTML('<style>{}</style>'.format(css))

# downloads vader_lexicon module from nltk for sentiment analysis
nltk.download('vader_lexicon')

# import modules from geopandas to plot map
import geopandas as gpd
from shapely.geometry import Point

# define regression model
model = LinearRegression()

# function to calculate arc distance between two locations
def calculate_min_distance(lat1,lat2,lon1,lon2):
  '''
  parameters:
    lat1: latitude value of location 1
    lat2: latitude value of location 2
    lon1: longitude value of location 1
    lon2: longitude value of location 2
  returns:
    float value which is the arc distance between the two coordinates
  '''
  # converts the coordinates to radians from degree
  lat1 = mat.radians(lat1)
  lat2 = mat.radians(lat2)
  long1 = mat.radians(lon1)
  long2 = mat.radians(lon2)
  lat = lat1 - lat2
  lon = long1 - long2
  # haversine formula to calculate the distance
  a = mat.sin(lat/2)**2 + mat.cos(lat2) * mat.cos(lat1) * mat.sin(lon/2)**2
  c = 2 * mat.asin(mat.sqrt(a)) 
  # multiply the arc distance with the radius of earth
  dist = 6378 * c
  # return the calculated distance
  return dist

# reads warehouse csv and stores in df
warehouse_df = pd.read_csv('/content/drive/Shareddrives/FIT5196_S1_2023/Assessment2/warehouses.csv')
warehouse_df

"""## **1: Missing Data**


---

This section of the assessnent identifies the missing data in the given file and fixes the data by imputing the values for all the missing columns.
"""

# reads the file into a dataframe
missing_data_df = pd.read_csv('/content/drive/Shareddrives/FIT5196_S1_2023/Assessment2/student_data/31940757_missing_data.csv')
missing_data_df.head(5)

# returns the attributes from the each column in the df
missing_data_df.info()

"""It is clear from the above code that we have three columns where we have null values:


*   **nearest_warehouse:** 100 records
*   **delivery_charges:** 50 records
*   **distance_to_nearest_warehouse:** 50 records







"""

# return list of indices where the filtered columns have null values
null_warehouses = missing_data_df[['nearest_warehouse','distance_to_nearest_warehouse']].isnull().any(axis=1)
# creates df containing the rows for above indices
loc_miss_df = missing_data_df[null_warehouses]
loc_miss_df.head()

# defines the latitudes and longitudes for the warehouses
nicolson_lat = warehouse_df.loc[0,'lat']
nicolson_long = warehouse_df.loc[0,'lon']
thompson_lat = warehouse_df.loc[1,'lat']
thompson_long = warehouse_df.loc[1,'lon']
bakers_lat = warehouse_df.loc[2,'lat']
bakers_long = warehouse_df.loc[2,'lon']
# iterates over the missing data df
for i in list(loc_miss_df.index.values):
  # defines the customer's locations
  long1 = loc_miss_df.loc[i, 'customer_long']
  lat1 = loc_miss_df.loc[i, 'customer_lat']
  # distance from nicolson
  dist_n = round(calculate_min_distance(lat1, nicolson_lat, 
                                        long1, nicolson_long), 5)
  # distance from thompson
  dist_t = round(calculate_min_distance(lat1, thompson_lat, 
                                        long1, thompson_long), 5)
  # distance from bakers
  dist_b = round(calculate_min_distance(lat1, bakers_lat, 
                                        long1, bakers_long), 5)
  # checks if the distance and nearest warehouse is missing 
  if (np.isnan(loc_miss_df.loc[i, 'distance_to_nearest_warehouse']) and 
      np.isnan(loc_miss_df.loc[i, 'nearest_warehouse'])):
    # checks for the least distance between the customers and the warehouses
    dist = min(dist_n, dist_t, dist_b)
    # if dist is equal to distance from Nickolson
    if dist == dist_n:
      # impute the missing values for Nickolson
      missing_data_df.at[i, "nearest_warehouse"] = "Nickolson"
      missing_data_df.at[i, "distance_to_nearest_warehouse"] = dist
    # if dist is equal to distance from Thompson
    elif dist == dist_t:
      # impute the missing values for Thompson
      missing_data_df.at[i, "nearest_warehouse"] = "Thompson"
      missing_data_df.at[i, "distance_to_nearest_warehouse"] = dist
    # if dist is equal to distance from Bakers
    else:
      # impute the missing values for Bakers
      missing_data_df.at[i, "nearest_warehouse"] = "Bakers"
      missing_data_df.at[i, "distance_to_nearest_warehouse"] = dist
  # checks if only distance is missing
  elif np.isnan(loc_miss_df.loc[i, 'distance_to_nearest_warehouse']):
    # checks for warehouse location in df
    if loc_miss_df.loc[i, 'nearest_warehouse'] == 'Nickolson':
      # impute the missing values
      missing_data_df.at[i, "distance_to_nearest_warehouse"] = dist_n
    elif loc_miss_df.loc[i, 'nearest_warehouse'] == 'Thompson':
      missing_data_df.at[i, "distance_to_nearest_warehouse"] = dist_t
    else:
      missing_data_df.at[i, "distance_to_nearest_warehouse"] = dist_b
  # checks if the nearest_warehouse is missing
  else:
    # checks if distance from Nickolson is equal to actual distance
    if dist_n == round(loc_miss_df.loc[i, 'distance_to_nearest_warehouse'],5):
      missing_data_df.at[i, "nearest_warehouse"] = "Nickolson"
    # checks if distance from thompson is equal to actual distance
    elif dist_t == round(loc_miss_df.loc[i, 'distance_to_nearest_warehouse'],5):
      missing_data_df.at[i, "nearest_warehouse"] = "Thompson"
    else:
      missing_data_df.at[i, "nearest_warehouse"] = "Bakers"
null_warehouses = missing_data_df[['nearest_warehouse','distance_to_nearest_warehouse']].isnull().any(axis=1)

null_warehouses.describe(include = 'O')

"""Since the delivery charges are linealry dependent in the season, distance, customer satisfaction and expedited delivery we need to make a linear model where delivery charge is treated as a dependent variable. Also the delivery charge in the data contains the discouted amount therefore we need to calculate the actual delivery charge before fitting the data to the model."""

discounted_price = 0
# iterates through the df
for i in list(missing_data_df.index.values):
  # calcuates delivery charges without the discount
  discounted_price = (missing_data_df.loc[i,"delivery_charges"])/(1 - (missing_data_df.loc[i,"delivery_discount"]/100))
  missing_data_df.at[i,"delivery_charges"] = discounted_price
missing_data_df.head()

# returns a df of indices where delivery charges are missing
missing_delivery = missing_data_df[['delivery_charges']].isnull().any(axis=1)
# subsets the main df with only the data where delivery charges are missing
price_miss_df = missing_data_df[missing_delivery]
price_miss_df.head()

# drops all the rows where delivery charge is null
lm_data_df = missing_data_df.dropna(subset=['delivery_charges'])
lm_data_df.head()

"""Since all the independent variables are categorical we use one-hot encoding method to encode the data to numerical equivalent. Choosing this technique will reduce the bias that might be caused by splitting the data into seasons and making four different models as the size of the training datset will significantly reduced."""

# encodes the seasons to a numeric value
season_encoded = pd.get_dummies(lm_data_df['season'], prefix='season')
# adds the encoded season to the actual df
lm_data_encoded = pd.concat([lm_data_df, season_encoded], axis=1)
# converts boolean to int binary type
lm_data_encoded['is_expedited_delivery'] = lm_data_encoded['is_expedited_delivery'].astype(int)
lm_data_encoded['is_happy_customer'] = lm_data_encoded['is_happy_customer'].astype(int)
lm_data_encoded.head()

# dataset containing independent variables
X = lm_data_encoded[['season_Spring', 'season_Summer', 'season_Autumn', 'season_Winter',
          'is_expedited_delivery', 'distance_to_nearest_warehouse', 'is_happy_customer']]
# dataset containing dependent variables
y = lm_data_encoded['delivery_charges']

# splits the dataset into test and train with a proportion of 20% and 80% respectively
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)

# train the model with the training dataset
model.fit(X_train, y_train)

# returns model accuracy for the test dataset
r_squared = model.score(X_test, y_test)

print('R-squared (coefficient of determination):', r_squared)

"""Since, the r-squared value is 96% > 95% we are going to use this model to predict the values of missing delivery charges."""

# encodes the seasons to a numeric value
season_encoded = pd.get_dummies(price_miss_df['season'], prefix='season')
# adds the encoded season to the actual df
price_miss_df_encoded = pd.concat([price_miss_df, season_encoded], axis=1)
# converts boolean to int binary type
price_miss_df_encoded['is_expedited_delivery'] = price_miss_df_encoded['is_expedited_delivery'].astype(int)
price_miss_df_encoded['is_happy_customer'] = price_miss_df_encoded['is_happy_customer'].astype(int)
price_miss_df_encoded.head()

# datasets from the missing df
X = price_miss_df_encoded[['season_Spring', 'season_Summer', 'season_Autumn', 'season_Winter',
          'is_expedited_delivery', 'distance_to_nearest_warehouse', 'is_happy_customer']]
y = price_miss_df_encoded['delivery_charges']
# predicts the delivery charges and impute the missing values
price_miss_df_encoded['delivery_charges'][price_miss_df_encoded['delivery_charges'].isnull()] = model.predict(X)
price_miss_df_encoded.head()

# iterates over missing df
for i in list(price_miss_df_encoded.index.values):
  # caluclates the delivery price by adding the discounted amount
  missing_data_df.at[i,'delivery_charges'] = price_miss_df_encoded.loc[i,"delivery_charges"]
missing_data_df.head()

# iterates over missing df
for i in list(missing_data_df.index.values):
  # caluclates the delivery price by adding the discounted amount
  discounted_price = round((missing_data_df.loc[i,"delivery_charges"])*(1 - (missing_data_df.loc[i,"delivery_discount"]/100)),5)
  missing_data_df.at[i,'delivery_charges'] = discounted_price
missing_data_df.head()

missing_data_df.info()

final_missing_data_df = missing_data_df.copy()

# creates a csv file from the imputed data df
final_missing_data_df.to_csv('/content/drive/MyDrive/Assessment2/31940757_missing_data_solution.csv',
index = False)

"""## **2: Cleaning Data**

---
This section deals with cleaning the dirty data csv file. Firstly the data is loaded from the csv file to a dataframe and analysed.
This step involves checking the shape of the df which gives us the number of rows and columns in the df.

Next we used df.info() which returns the description of column, its datatype and if or nor it contains null values.

After this we perform Exploratory Data Analysis on the df to check for the statistics of the numerical data which includes mean, max, min, etc. values from the columns. Categorical data its count, freq,top and unique values from the columns.
"""

# change the working directory to the local folder
os.chdir("/content/drive/MyDrive")
#os.mkdir("./Assessment2")

os.chdir("./Assessment2")

# reads the dirty data file in the shared drive and stores in the data frame
dirty_data_raw = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2023/Assessment2/student_data/31940757_dirty_data.csv")
# prints the top rows from the df
dirty_data_raw.head()

# prints shape of the dataframe
print (dirty_data_raw.shape)

"""* Categorical Variables: <font color="green">nearest_warehouse</font>, <font color="green">shopping_cart</font>, <font color="green">season</font>, <font color="green">is_expedited_delivery</font>, <font color="green">latest_customer_review</font>, <font color="green">is_happy_customer</font>
    * Ordinal variables: <font color="green">order_id</font>, <font color="green">customer_id</font>, <font color="green">date</font>
* Numerical Variables: <font color="red">order_price</font>, <font color="red">delivery_charges</font>, <font color="red">customer_lat</font>, <font color="red">customer_long</font>, <font color="red">delivery_discount</font>, <font color="red">distance_to_nearest_warehouse</font>
"""

# gives description of the columns in df
dirty_data_raw.info()

# returns statical inference for the numerical data
dirty_data_raw.describe()

"""Latitude Ranges between [-90, 90] while Longtitude ranges between [-180, 180]. Also Melbourne is located at -37.840935, 144.946457. Since we are given that the store is based in Melbourne, the latitude and longitude values should be within the proximity of Melbourne therefore the Min and Max of Latitude and Longitude needs to be checked."""

# returns the statical inference for the categorical data
dirty_data_raw.describe(include=['O'])

"""

1.   We are given that there are three warehouses but there are six unique values.
2.   We only experience four season but there are seven unique values.


"""

# adds new column to the dataframe with a default value False
dirty_data_raw["dirty_flag"] = False
dirty_data_raw.head()

# returns the unique values in the column season with their count of occurance
dirty_data_raw.season.value_counts()

# returns the filtered dataframe 
filtered_seasons = dirty_data_raw[(dirty_data_raw["season"] == 'autumn') |
                                  (dirty_data_raw["season"] == 'summer') |
                                  (dirty_data_raw["season"] == 'spring')]
filtered_seasons

#  replaces cell containing the keys with the values in the dict
dirty_data_raw['season'] = dirty_data_raw.season.replace({"autumn":"Autumn","summer":"Summer","spring":"Spring"})

# retruns value count after modifying thr dirty data
dirty_data_raw.season.value_counts()

# list of index values for modified records 
index_values = list(filtered_seasons.index.values)
for i in index_values:
  # sets the dirty flag column to true for the records that are modified.
  dirty_data_raw.at[i, "dirty_flag"] = True

# returns the modified records from the df
dirty_data_raw[dirty_data_raw["dirty_flag"] == True]

#returns the unique values in the column nearest_warehouse with their count of occurance
dirty_data_raw.nearest_warehouse.value_counts()

# filtered df containing the dirty values for nearest_warehouse
filtered_warehouses = dirty_data_raw[(dirty_data_raw["nearest_warehouse"] == 'bakers') |
                                  (dirty_data_raw["nearest_warehouse"] == 'nickolson') |
                                  (dirty_data_raw["nearest_warehouse"] == 'thompson')]
filtered_warehouses

# replaces the dirty values in nearest_warehouse with the actual values
dirty_data_raw['nearest_warehouse'] = dirty_data_raw.nearest_warehouse.replace({"nickolson":"Nickolson","bakers":"Bakers","thompson":"Thompson"})

# returns the value count after modification
dirty_data_raw.nearest_warehouse.value_counts()

# sets the dirty flag as True for the modified data
index_values = list(filtered_warehouses.index.values)
for i in index_values:
  dirty_data_raw.at[i, "dirty_flag"] = True

"""In this section we will clean the geospatial data i.e. the coordinates of the customers. We know that the customers belong to Melbourne which implies that we need to look for data points that lies outside Melbourne.

We zip the latitude and longitude values from the df in a list. GeoDataFrame object is created by passing the df and the coordinates as the parameters.

Map is constructed by reading the path in naturalearth_lowres in geopandas dataset. This stores a low res map to the variable which is used to plot the datapoints on.
"""

# defines the coordinates of the customers
coordinates = [Point(xy) for xy in zip(dirty_data_raw['customer_long'], dirty_data_raw['customer_lat'])]
# geodataframe object for spatial opreations
gdf = gpd.GeoDataFrame(dirty_data_raw, geometry = coordinates)
# map object containing low resolution map of the world
map = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
# plots the maps containg the datapoints from df
gdf.plot(ax = map.plot(figsize = (10, 10)), color = 'red', markersize = 30)

"""It is clearly observed that the there are observations recorded outside Melbourne which seems to be way beyond our target location.

We will check for the values by filtering out the df based on values greater than -37 for latitude and less than 144 for the longitudes.
"""

geospatial_dirty_df = dirty_data_raw[(dirty_data_raw["customer_lat"] > -37) & 
                                     (dirty_data_raw["customer_long"] < 144)]
geospatial_dirty_df

"""From the above data it can be inferred that the values for latitude and longitude for the records was interchanged. We can fix them by again interchanging the values for both the columns."""

# interchanges the values of the records in the above df and 
#sets the dirty flag as true after modifying the data
index_values = list(geospatial_dirty_df.index.values)
for i in index_values:
  dirty_data_raw.at[i, "customer_lat"], dirty_data_raw.at[i, "customer_long"] = dirty_data_raw.at[i, "customer_long"], dirty_data_raw.at[i, "customer_lat"]
  dirty_data_raw.at[i, "dirty_flag"] = True

# verifying the modified records
dirty_data_raw.loc[[30, 149, 162, 441, 458]]

# filter for data that has already been cleaned
filtered_data = dirty_data_raw[dirty_data_raw["dirty_flag"] == True]
filtered_data.head()

# list to store the product names
list_items = []
# loops through the column shopping cart
for data in filtered_data["shopping_cart"]:
  # regex to get the name of the item
  matches = re.findall(r'(?:\(\')(.*?)(?:\'\,)',data)
  if matches:
    for items in matches:
      # adds the regex match to the list
      list_items.append(items)
list_items
# set containing name of the unique products
item_set = set(list_items)
print(len(item_set))

"""In the above code block we can see we have 10 unique products from the retailers in the data we have already cleaned out. Since there can only be single error in a row we can assume the shopping cart total would be equal to the order_price, therefore we use this data to get the price of the individiual products."""

# df to quantities of the products purchased in every order
order_df = pd.DataFrame(columns = sorted(list(item_set)), index = filtered_data.index)
# adds new column to the df
order_df["order_price"] = 0
order_df.head()

# adds the value of order_price from clean dataframe
for i in list(filtered_data.index.values):
  order_df.at[i, "order_price"] = filtered_data.at[i, "order_price"]
order_df.head()

for i in list(filtered_data.index.values):
  # regex to find the peoduct name and item quantity
  matches = re.findall(r'(?:\(\')(.*?)(?:\'\, )(\d*)',filtered_data.loc[i, "shopping_cart"])
  for products in matches:
    # adds the quantity from match object at the index of product name
    order_df.at[i, products[0]] = products[1]
# all na values are set as 0
order_df = order_df.fillna(0)
order_df.head()

"""Since we have 10 products in total we fetch the top 10 rows from the order df in order to 10 linear equations and solve them using numpy module to get the price values for individual products."""

# stores the top 10 rows from order df
alg_df = order_df.head(10)

# creates an n dimension array from the values of each column in the df
cart_arr = np.array(alg_df.iloc[:, :-1].values, dtype=np.int64)
# creates n dimension array for the order price
order_arr = np.array(alg_df.iloc[:, -1].values, dtype=np.int64)

order_arr

cart_arr

# solves linear equation and stors the result 
item_prices = np.linalg.solve(cart_arr, order_arr)
item_prices

# list containing the product names
prod_name = order_df.columns[:-1]
# zips the products as keys and prices as values in a dict
prices_value = dict(zip(prod_name, item_prices))
prices_value

# df containing the dirty data
dirty_data = dirty_data_raw[dirty_data_raw["dirty_flag"] == False]
dirty_data.head()

# check if the direty data contains 10 items or not
list_items = []
for data in dirty_data["shopping_cart"]:
  matches = re.findall(r'(?:\(\')(.*?)(?:\'\,)',data)
  if matches:
    for items in matches:
      list_items.append(items)
list_items
item_set = set(list_items)
item_set

order_df = pd.DataFrame(columns = sorted(list(item_set)), index = dirty_data.index)
order_df["total_price"] = 0
order_df.head()

for i in list(dirty_data.index.values):
    matches = re.findall(r'(?:\(\')(.*?)(?:\'\, )(\d*)',dirty_data.loc[i, "shopping_cart"])
    for products in matches:
      order_df.at[i, products[0]] = products[1]
    order_df.at[i, "order_price"] = dirty_data.at[i, "order_price"]
order_df = order_df.fillna(0)

order_df.head()

for i in list(order_df.index.values):
  # for each row in the df sets the price of the product and multiply with the quantity
  alcon_10 = float(order_df.loc[i,'Alcon 10'])* float(prices_value['Alcon 10'])
  candle_iferno = float(order_df.loc[i,'Candle Inferno'])* float(prices_value['Candle Inferno'])
  lucent_330s = float(order_df.loc[i,'Lucent 330S'])* float(prices_value['Lucent 330S'])
  olivia_x460 = float(order_df.loc[i,'Olivia x460'])* float(prices_value['Olivia x460'])
  thunder_line = float(order_df.loc[i,'Thunder line'])* float(prices_value['Thunder line'])
  toshika_750 = float(order_df.loc[i,'Toshika 750'])* float(prices_value['Toshika 750'])
  universe_note = float(order_df.loc[i,'Universe Note'])* float(prices_value['Universe Note'])
  iassist_line = float(order_df.loc[i,'iAssist Line'])* float(prices_value['iAssist Line'])
  istream = float(order_df.loc[i,'iStream'])* float(prices_value['iStream'])
  peartv = float(order_df.loc[i,'pearTV'])* float(prices_value['pearTV'])

  # sum of the shopping cart total
  total_price = (alcon_10 + candle_iferno + lucent_330s + 
                               olivia_x460 + thunder_line + toshika_750 + 
                               universe_note + iassist_line + 
                               iassist_line + istream + peartv)
  order_df.at[i,"total_price"] = total_price
order_df.head()

# returns the df where actual price is not equal to the order_price
cart_dirty_df = order_df[round(order_df['total_price'],3) != round(order_df["order_price"],3)]
cart_dirty_df.head()

# replaces the order_price values with the calculated total price
for i in list(cart_dirty_df.index.values):
  dirty_data_raw.at[i, 'order_price'] = cart_dirty_df.loc[i, 'total_price']
  # sets dirty flag as true for the modified data
  dirty_data_raw.at[i, 'dirty_flag'] = True
clean_data_df = dirty_data_raw.copy()
clean_data_df.head()

# checks and return the df where date is not in standard format
invalid_dates = clean_data_df[pd.to_datetime(clean_data_df['date'], 
                                             format='%Y-%m-%d', 
                                             errors='coerce').isna()]
invalid_dates

for i in list(invalid_dates.index.values):
  try:
    # changes the date column to datetime format and identifies the actual format itself
    clean_data_df.at[i,'date'] = pd.to_datetime(clean_data_df.loc[i,'date'], 
                               infer_datetime_format=True, errors = 'raise')
    clean_data_df.at[i,'dirty_flag'] = True
  except:
    # for the format that throws error while conversion
    clean_data_df.at[i, 'date'] = pd.to_datetime(clean_data_df.loc[i,'date'], 
                                                 format='%Y-%d-%m')
    clean_data_df.at[i,'dirty_flag'] = True
# strips the time from datetime type object and returns the date part
clean_data_df['date'] = pd.to_datetime(clean_data_df['date']).dt.strftime('%Y-%m-%d')

clean_data_df.loc[109,'date']

clean_data_df.loc[314,'date']

"""**2.1: Calculate Arc Distance**

---
This section is used to calculate the distance between the customer and their nearest warehouse. The calculated distance is then compared to the one in the df and checked for its accuracy.
"""

# df contaning order_id, nearest_warehouse and customer coordinates for the remaining dirty data
loc_df = clean_data_df.loc[clean_data_df['dirty_flag'] == False, 
                           ['order_id', 'nearest_warehouse', 'customer_lat', 'customer_long','distance_to_nearest_warehouse','dirty_flag']]
loc_df.head()

# defines the latitudes and longitudes for the warehouses
nicolson_lat = warehouse_df.loc[0,'lat']
nicolson_long = warehouse_df.loc[0,'lon']
thompson_lat = warehouse_df.loc[1,'lat']
thompson_long = warehouse_df.loc[1,'lon']
bakers_lat = warehouse_df.loc[2,'lat']
bakers_long = warehouse_df.loc[2,'lon']
# iterates over the missing data df
for i in list(loc_df.index.values):
  # defines the customer's locations
  long1 = loc_df.loc[i, 'customer_long']
  lat1 = loc_df.loc[i, 'customer_lat']
  # distance from nicolson
  dist_n = round(calculate_min_distance(lat1, nicolson_lat, 
                                        long1, nicolson_long), 5)
  # distance from thompson
  dist_t = round(calculate_min_distance(lat1, thompson_lat, 
                                        long1, thompson_long), 5)
  # distance from bakers
  dist_b = round(calculate_min_distance(lat1, bakers_lat, 
                                        long1, bakers_long), 5)
  
  # checks for the least distance between the customers and the warehouses
  dist = min(dist_n, dist_t, dist_b)
  if dist == dist_n:
    if not loc_df.loc[i, 'distance_to_nearest_warehouse'] == dist:
      loc_df.at[i, 'distance_to_nearest_warehouse'] = dist
      loc_df.at[i, 'dirty_flag'] = True
    if not loc_df.loc[i, 'nearest_warehouse'] == 'Nickolson':
      loc_df.at[i, 'nearest_warehouse'] = 'Nickolson'
      loc_df.at[i, 'dirty_flag'] = True
  elif dist == dist_t:
    if not loc_df.loc[i, 'distance_to_nearest_warehouse'] == dist:
      loc_df.at[i, 'distance_to_nearest_warehouse'] = dist
      loc_df.at[i, 'dirty_flag'] = True
    if not loc_df.loc[i, 'nearest_warehouse'] == 'Thompson':
      loc_df.at[i, 'nearest_warehouse'] = 'Thompson'
      loc_df.at[i, 'dirty_flag'] = True
  else:
    if not loc_df.loc[i, 'distance_to_nearest_warehouse'] == dist:
      loc_df.at[i, 'distance_to_nearest_warehouse'] = dist
      loc_df.at[i, 'dirty_flag'] = True
    if not loc_df.loc[i, 'nearest_warehouse'] == 'Bakers':
      loc_df.at[i, 'nearest_warehouse'] = 'Bakers'
      loc_df.at[i, 'dirty_flag'] = True
loc_df.head()

loc_df.describe()

for i in list(loc_df.index.values):
  # checks if the calculated distance is equal to the given distance
  if loc_df.loc[i, 'dirty_flag'] == True:
    # if not changes the values to calculated distance
    clean_data_df.at[i, 'distance_to_nearest_warehouse'] = loc_df.loc[i,'distance_to_nearest_warehouse']
    clean_data_df.at[i, 'nearest_warehouse'] = loc_df.loc[i,'nearest_warehouse']
    clean_data_df.at[i, 'dirty_flag'] = True
clean_data_df.head()

"""**2.2: Sentiment Analysis**


---
In this section we perform sentiment analysis on the feeback provided by the customer for their last purchase. We achieve this by using vader_lexicon module from nltk library which performs sentiment analysis and provides the polarity score for it. A customer is happy with their purchase when the compund score is greater than 0.05.

"""

# creates a df with remaining dirty data
feedback_df = clean_data_df.loc[clean_data_df['dirty_flag']== False,['order_id','latest_customer_review']]
# adds column for polarity_score to df
feedback_df['polarity_score'] = 0
# adds new column which is the happiness flag
feedback_df['is_happy'] = False
feedback_df.head()

# initializes sentiment analyzer
sent_analyzer = SentimentIntensityAnalyzer()
# variable to hold boolean for hapoiness index
pos_fdbk = False
# loops through the df
for i in list(feedback_df.index.values):
  # feeback provided by the customer
  feedback = feedback_df.loc[i,'latest_customer_review']
  # return the polarity score dict for the review
  score = sent_analyzer.polarity_scores(feedback)
  # checks the compound score with the threshold value
  if score['compound'] >= 0.05:
    # sets happiness flag as true
    pos_fdbk = True
  else:
    pos_fdbk = False
  # adds the compound score to the df
  feedback_df.at[i,'polarity_score'] = score['compound']
  # adds happiness flag to the df
  feedback_df.at[i,'is_happy'] = pos_fdbk
feedback_df.head()

# loops through feedback df
for i in list(feedback_df.index.values):
  # checks if actual data is same as the calculated
  if feedback_df.loc[i, 'is_happy'] != clean_data_df.loc[i,'is_happy_customer']:
    # if not changes the value to calculated field
    clean_data_df.at[i,'is_happy_customer'] = feedback_df.loc[i, 'is_happy']
    # sets dirty flag as true for the modified data
    clean_data_df.at[i,'dirty_flag'] = True

"""**2.3: Expedited delivery**


---

In this section we will check if the is_expedited_delivery correct in the data. We know that there is linear depedency between expedited delivery and delivery charges, also delivery charges are always right, also season, distance_from_nearest_warehouse and is_happy has already been fixed therefore we can assume there might be some noise in the delivery charges when predicted using the model described in Missing Data Section. This noise will cause outliers which are identified using residual technique. After identifying the outliers we will modify the value of the is_expedited_delivery to opposite of what it actually is.


"""

# defines the testing data
lm_df = clean_data_df.loc[clean_data_df['dirty_flag']== False].copy()
# calculate the delivery charges without the discount in testing data
for i in list(lm_df.index.values):
  discounted_price = (lm_df.loc[i,"delivery_charges"])/(1 - (lm_df.loc[i,"delivery_discount"]/100))
  lm_df.at[i,"discounted_delivery"] = discounted_price
lm_df.head()

# encodes the seasons to a numeric value
season_encoded = pd.get_dummies(lm_df['season'], prefix='season')
# adds the encoded season to the actual df
lm_df_encoded = pd.concat([lm_df, season_encoded], axis=1)
# converts boolean to int binary type
lm_df_encoded['is_expedited_delivery'] = lm_df_encoded['is_expedited_delivery'].astype(int)
lm_df_encoded['is_happy_customer'] = lm_df_encoded['is_happy_customer'].astype(int)
lm_df_encoded.head()

# predicts the values for delivery charges based on the independent variables
lm_df_encoded["predicted_delivery"]= model.predict(lm_df_encoded[['season_Spring', 'season_Summer', 'season_Autumn', 'season_Winter',
          'is_expedited_delivery', 'distance_to_nearest_warehouse', 'is_happy_customer']])

# calculates resilduals which is the difference between the actual and predicted values
# adds the residuals to the df
lm_df_encoded["residuals"] = (lm_df_encoded['discounted_delivery'] - lm_df_encoded['predicted_delivery'])

# statistical inference of the residual column
stat_info = lm_df_encoded[["residuals"]].describe()
stat_info

# lower threshold for outlier
lower_limit = float(stat_info.loc["25%"] - 1.5*(stat_info.loc["75%"] - stat_info.loc["25%"]))
# upper threshold for outlier

upper_limit = float(stat_info.loc["75%"] + 1.5*(stat_info.loc["75%"] - stat_info.loc["25%"]))
print(f'upper limit: {upper_limit} :: lower limit: {lower_limit}')

# returns the list of indices which are outliers in the data
outliers = list(lm_df_encoded[(lm_df_encoded['residuals'] < lower_limit) | (lm_df_encoded['residuals'] > upper_limit)].index)
len(outliers)

# iterates through the list
for i in outliers:
  # if is_expedited delivery is true
  if clean_data_df.loc[i, "is_expedited_delivery"]:
    # change it to false
    clean_data_df.at[i, "is_expedited_delivery"] = False
    # set dirty flag as True
    clean_data_df.at[i, "dirty_flag"] = True
  else:
    # change it to True
    clean_data_df.at[i, "is_expedited_delivery"] = True
    # set dirty flag to true
    clean_data_df.at[i, "dirty_flag"] = True
clean_data_df.head()

# creates a copy of clean data
final_clean_df = clean_data_df.iloc[:, :-1].copy()
final_clean_df.head()

# creates a csv file from the clean data df
final_clean_df.to_csv('/content/drive/MyDrive/Assessment2/31940757_dirty_data_solution.csv',
index = False)

"""## **3: Outlier Data**


---
This section of the assessment deals with outlier handling and removal. Since the delivery charges are linearly depedent in season, distance from the warehouse, satisfaction from the last purchase and expedited delivery, we use a linear regression model to predict the delivery price. Then we use residual method approach to figure out the difference between the actual and predicted values and check if the values fit within the Inter Quartile Range. The datapoints outside IQR both at upper and lower ends are treated as outliers.

"""

# loads data from csv into dataframe
data_df = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2023/Assessment2/student_data/31940757_outlier_data.csv")
# prints top rows from the dataframe
data_df.head()

# boxplot depicting the distribution of the delivery charges
data_df.boxplot(column = 'delivery_charges')

# calculation for delovery charges without the discount
data_df["discounted_delivery"] = 0
discounted_price = 0
# loop to iterate through the outlier data
for i in list(data_df.index.values):
  discounted_price = (data_df.loc[i,"delivery_charges"])/(1 - (data_df.loc[i,"delivery_discount"]/100))
  # adds the calulcated delivery price to the df
  data_df.at[i,"discounted_delivery"] = discounted_price
data_df.head()

# creates a copy of the df
outlier_df = data_df.copy()

# encodes the seasons to binary values
season_encoded = pd.get_dummies(data_df['season'], prefix='season')
# adds the encoded seasons to the outlier df
data_encoded_df = pd.concat([data_df, season_encoded], axis=1)
# cast the boolean columns to numerical binary data
data_encoded_df['is_expedited_delivery'] = data_encoded_df['is_expedited_delivery'].astype(int)
data_encoded_df['is_happy_customer'] = data_encoded_df['is_happy_customer'].astype(int)
# dataset containing independent variables
X = data_encoded_df[['season_Spring', 'season_Summer', 'season_Autumn', 'season_Winter',
          'is_expedited_delivery', 'distance_to_nearest_warehouse', 'is_happy_customer']]
# dataset containing the dependent variable
y = data_encoded_df['discounted_delivery']

data_encoded_df.head()

# predicts the values for delivery charges based on the independent variables
outlier_df["predicted_delivery"]= model.predict(data_encoded_df[['season_Spring', 'season_Summer', 'season_Autumn', 'season_Winter',
          'is_expedited_delivery', 'distance_to_nearest_warehouse', 'is_happy_customer']])

# calculates resilduals which is the difference between the actual and predicted values
# adds the residuals to the df
outlier_df["residuals"] = (outlier_df['discounted_delivery'] - outlier_df['predicted_delivery'])

outlier_df.head()

# boxplot showing distribution of the residuals
outlier_df.boxplot(column='residuals')

"""We can see their are quite a number of datapoints which are beyond IQR. The thresdhold value for data is 1.5* IQR beyond which the data is considered as outliers. Therefore we check for statiscal infromation for this data and check for outliers. IQR is given as difference between the first quartile(25%) and third quartile (75%).

"""

stat_info = outlier_df[["residuals"]].describe()
stat_info

lower_limit = float(stat_info.loc["25%"] - 1.5*(stat_info.loc["75%"] - stat_info.loc["25%"]))
upper_limit = float(stat_info.loc["75%"] + 1.5*(stat_info.loc["75%"] - stat_info.loc["25%"]))
print(f'upper limit: {upper_limit} :: lower limit: {lower_limit}')

outliers = list(outlier_df[(outlier_df['residuals'] < lower_limit) | (outlier_df['residuals'] > upper_limit)].index)
len(outliers)

"""We have got 40 outliers in the data which we need to remove."""

final_outlier_data = data_df.drop(outliers, axis=0)
final_outlier_data = final_outlier_data.reset_index(drop=True)

final_outlier_data.shape

final_outlier_data.head()

# creates a csv file from the clean data df
final_outlier_data.to_csv('/content/drive/MyDrive/Assessment2/31940757_outlier_data_solution.csv',
index = False)